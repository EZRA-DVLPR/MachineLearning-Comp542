{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project - Cat Image Classification\n",
    "\n",
    "## Group 4: Isaiah Martinez, Joycelyn Tuazon\n",
    "## Mrs. Lord - Comp 542 Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guides that may provide insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count objects\n",
    "#https://new.pythonforengineers.com/blog/image-and-video-processing-in-python/\n",
    "\n",
    "#Basic outline for image classificaiton. See youtube video too.\n",
    "#https://www.youtube.com/watch?v=il8dMDlXrIE\n",
    "#https://github.com/computervisioneng/image-classification-python-scikit-learn/blob/master/main.py\n",
    "\n",
    "#Image classification with Tensorflow and Keras\n",
    "#https://www.tensorflow.org/tutorials/images/classification\n",
    "\n",
    "#Load and Preprocess Images\n",
    "#https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Examples with Code to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good to model after\n",
    "# Also in the Conv2D section\n",
    "# https://www.kaggle.com/code/shaileshkakade/image-classification-cat-dog-panda\n",
    "\n",
    "# plots vertical/horizontal edge detection\n",
    "# https://www.kaggle.com/code/dianalaveena/cnn-keras-image-classification\n",
    "# Consider KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/dongduongminh/cnn-deep-learning-image-classification\n",
    "# https://www.kaggle.com/code/nasrinjahanripa/animal-images-classification-cnn\n",
    "# https://www.kaggle.com/code/corneliustantius/classification-with-keras-cnn\n",
    "# SEE `explaining-line.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/chenjiarui2018/cats-dogs-and-pandas-convnext#3.-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/arpita12/cat-dog-panda\n",
    "# https://www.kaggle.com/code/rwt1998/animal-classification\n",
    "# https://www.kaggle.com/code/atmaraiprince/animal-image-dataset\n",
    "# https://www.kaggle.com/code/kkamal2003/animal-images-cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/shaileshkakade/image-classification-cat-dog-panda\n",
    "# https://www.kaggle.com/code/etienne1976/animal-image-classification-with-keras\n",
    "# https://www.kaggle.com/code/bygbrains/dog-cat-pandas-image-classifier\n",
    "# https://www.kaggle.com/code/dianalaveena/cnn-keras-image-classification#5.-Model-Building-and-Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Res Net 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/nasrinjahanripa/animal-images-classification-cnn\n",
    "# https://www.kaggle.com/code/profmedo/image-classification-cnn-with-resnet\n",
    "# https://www.kaggle.com/code/kkamal2003/resnet-for-classification\n",
    "# https://www.kaggle.com/code/arshnoor7389/resnet-for-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI - CLIP\n",
    "## Dont use CLIP as project. CLIP may provide insight or be compared to, but not submitted for grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GITHUB LINK:\n",
    "# https://github.com/openai/CLIP/blob/main/model-card.md#model-card-clip\n",
    "# https://www.kaggle.com/code/kimchanyoung/zero-shot-prediction-using-openai-s-clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119. 120. 120. ...  37.  37.  38.]\n",
      "(250000,)\n",
      "# cat imgs 994\n",
      "# dog imgs 990\n",
      "# panda imgs 968\n",
      "total dataset size is 2952\n"
     ]
    }
   ],
   "source": [
    "#Resize all images into arrays\n",
    "import projHelper as pjH\n",
    "\n",
    "cats_resized = pjH.getResizedGrayscaleFlattenedArrays(\"animals_cleaned/cats/\")\n",
    "dogs_resized = pjH.getResizedGrayscaleFlattenedArrays(\"animals_cleaned/dogs/\")\n",
    "panda_resized = pjH.getResizedGrayscaleFlattenedArrays(\"animals_cleaned/panda/\")\n",
    "\n",
    "#each image is an array of 750,000 (500 * 500 * 3)\n",
    "print(cats_resized[0])\n",
    "print(cats_resized[0].shape)\n",
    "\n",
    "#Resized cat/dog/panda images into a single array of size 994/990/968\n",
    "print(\"# cat imgs\", len(cats_resized))\n",
    "print(\"# dog imgs\", len(dogs_resized))\n",
    "print(\"# panda imgs\", len(panda_resized))\n",
    "print(\"total dataset size is\", len(cats_resized) + len(dogs_resized) + len(panda_resized))\n",
    "\n",
    "#14 second runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the modified images into animals_cleaned_GS directory\n",
    "import projHelper as pjH\n",
    "\n",
    "pjH.saveModifiedImages(\"animals_cleaned/cats/\", \"animals_cleaned_GS/cats/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create labels and combine data into a single array\n",
    "\n",
    "#insert labels into array\n",
    "labels = [\"cat\"] * len(cats_resized) + [\"dog\"] * len(dogs_resized) + [\"panda\"] * len(panda_resized)\n",
    "\n",
    "#combine data into a single array\n",
    "data_resized = [*cats_resized, *dogs_resized, *panda_resized]\n",
    "\n",
    "#compare sizes of labels and data_resized since they should be the same\n",
    "#expected: True\n",
    "print(len(data_resized) == len(labels))\n",
    "\n",
    "#testing labels\n",
    "#expected: cat cat dog dog panda panda\n",
    "print(labels[0], labels[993], labels[994], labels[1983], labels[1984], labels[2951])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#80/20 split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(data_resized, labels, test_size = 0.2)\n",
    "\n",
    "print(len(xTrain), xTrain, \"\\n\")\n",
    "print(len(yTrain), yTrain, \"\\n\")\n",
    "print(len(xTest), xTest, \"\\n\")\n",
    "print(len(yTest), yTest, \"\\n\")\n",
    "\n",
    "#each array in xTrain and xTest is of type: float32\n",
    "#the max value from xTrain and xTest is 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "#                       Notes                           #\n",
    "#########################################################\n",
    "\n",
    "# print out Accuracy, Specificity, Recall, F1 Score. If possible, also the Confusion Matrix\n",
    "\n",
    "# Analyze model for Over/Underfitting so that it can be reduced\n",
    "\n",
    "# Use/find an alg to balance the class vs not class for images (Cat vs Non-Cat)\n",
    "\n",
    "# possibly convert rgb images to grayscale using GLCM\n",
    "#      might want to compare rgb images and grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isaiah Martinez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the svm classifier with no parameter changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed dataset into model\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel = 'linear', cache_size = 2000, max_iter = 5000, decision_function_shape = 'ovr', verbose = True)\n",
    "clf.fit(xTrain, yTrain)\n",
    "\n",
    "#15 minute runtime 750,000\n",
    "\n",
    "#8 minute runtime 250,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict and obtain metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "yPred = clf.predict(xTest)\n",
    "\n",
    "acc = accuracy_score(yTest, yPred)\n",
    "f1 = f1_score(yTest, yPred, average = 'weighted')\n",
    "prec = precision_score(yTest, yPred, average = 'weighted')\n",
    "recall = recall_score(yTest, yPred, average = 'weighted')\n",
    "\n",
    "print(acc, f1, prec, recall)\n",
    "\n",
    "#4 minute runtime (750,000)\n",
    "\n",
    "#metrics:\n",
    "#0.5617597292724196 0.570465072568459 0.591411754950014 0.5617597292724196\n",
    "\n",
    "#2 minute runtime (250,000)\n",
    "\n",
    "#metrics:\n",
    "#0.44839255499153974 0.4560295854772547 0.4798984919706284 0.44839255499153974"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline + GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###     Final Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('classifier', svm.SVC(cache_size = 16000, max_iter = 5000, decision_function_shape = 'ovr', verbose = True))\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'classifier__C' : [5, 1]\n",
    "}\n",
    "\n",
    "gridSearch = GridSearchCV(pipeline, params, cv = 5, n_jobs = 1, verbose = 4)\n",
    "\n",
    "gridSearch.fit(xTrain, yTrain)\n",
    "\n",
    "#1750 minute runtime (initial 750,000) (250 fits)\n",
    "#100 minute runtime   (initial 250,000) (1 fit)\n",
    "#417 minute runtime   (initial 250,000) (15 fits)\n",
    "#1051 minute runtime   (initial 250,000) (45 fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the best params and best model found from Grid Search\n",
    "gridSearchParams = gridSearch.best_params_\n",
    "gridSearchModel = gridSearch.best_estimator_\n",
    "\n",
    "print(gridSearchParams, gridSearchModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction and metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "yPred = gridSearchModel.predict(xTest)\n",
    "\n",
    "acc = accuracy_score(yTest, yPred)\n",
    "f1 = f1_score(yTest, yPred, average = 'weighted')\n",
    "prec = precision_score(yTest, yPred, average = 'weighted')\n",
    "recall = recall_score(yTest, yPred, average = 'weighted')\n",
    "\n",
    "print(acc, f1, prec, recall)\n",
    "#4 minute runtime\n",
    "\n",
    "#initial run stats:\n",
    "#0.5854483925549916 0.5885099209700294 0.5983072203381199 0.5854483925549916\n",
    "\n",
    "#12 minute runtime 1 fit\n",
    "\n",
    "#initial run stats: 1 fit\n",
    "#0.6091370558375635 0.6054727828929725 0.6030864613210003 0.6091370558375635\n",
    "\n",
    "#12 minute runtime 15 fits\n",
    "\n",
    "#initial run stats: 1 fit\n",
    "#0.6328257191201354 0.6299543290220251 0.6287454462588863 0.6328257191201354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(yTest, yPred, labels = gridSearchModel.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = gridSearchModel.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Prediction CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ViT-B/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device = device)\n",
    "\n",
    "img = preprocess(Image.open(\"animals_cleaned/cats/cats_00001.jpg\")).unsqueeze(0).to(device)\n",
    "\n",
    "animals = ['cat', 'dog', 'panda']\n",
    "\n",
    "text = clip.tokenize(animals).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    imageFeats = model.encode_image(img)\n",
    "    textFeats = model.encode_text(text)\n",
    "\n",
    "    logitsPerImg, logitsPerText = model(img, text)\n",
    "    probs = np.array(logitsPerImg.softmax(dim = -1).cpu().numpy())\n",
    "\n",
    "label = np.argmax(probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9885767\n",
      "cat\n",
      "[0.9885767  0.00973916 0.00168417]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(probs[0][label])\n",
    "print(animals[label])\n",
    "print(probs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RN50x16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 630M/630M [00:49<00:00, 13.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99359655\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"RN50x16\", device = device)\n",
    "\n",
    "img = preprocess(Image.open(\"animals_cleaned/cats/cats_00001.jpg\")).unsqueeze(0).to(device)\n",
    "\n",
    "animals = ['cat', 'dog', 'panda']\n",
    "\n",
    "text = clip.tokenize(animals).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    imageFeats = model.encode_image(img)\n",
    "    textFeats = model.encode_text(text)\n",
    "\n",
    "    logitsPerImg, logitsPerText = model(img, text)\n",
    "    probs = np.array(logitsPerImg.softmax(dim = -1).cpu().numpy())\n",
    "\n",
    "label = np.argmax(probs[0])\n",
    "\n",
    "print(probs[0][label])\n",
    "print(animals[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On all Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joycelyn Tuazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here...\n",
    "\n",
    "import keras\n",
    "import projHelper as pjH\n",
    "import pandas as pd\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
